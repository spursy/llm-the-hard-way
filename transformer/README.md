# Transformer


事实上，可以把 Transformer 想象成一个 "聪明的翻译官"，两个主要部分组成：编码器和解码器

- 编码器：就像先听懂一句话的各个部分，把它们的意思 "拆解" 并转换成一种内部表示
- 解码器：在根据这种内部表示生成新的输出，比如翻译成另一种语言的句子

```
整个过程的关键在于 "注意力机制"，也就是模型在处理每个单词时，回去关注输入句子中所有其它单词的重要性。通过层层叠加（堆叠多个编码器层和解码器层），模型不断提炼和整合信息，使得最终输出更加准确和流畅
```

## Transformer 架构的详细解析

### 架构整体与堆叠结构

#### 编码器

- 一般由 N 层相同结构的编码器单元组成

**每层包括：**

- 多头自注意力（Multi-head Self-Attention）：让模型在当前输入中寻找各个位置件的联系
- 前馈神经网络（Feed-Forward Network, FFN）：对每个位置的表示进行进一步非线性变换
- 每个子层后都有 残差连接（Residual Connection）和归一化层（Layer Normalization），以保持信息稳定传递

#### 解码器

- 同样由 N 层堆叠而成，但每层多了一个额外的注意力子层，用于结合编码器的输出
- 自注意力层：和编码其类似，但在训练时采用遮蔽机制，防止看到未来的输出信息
- 编码器 - 解码器注意力层：使解码器能 "查找" 编码器的输出信息，获取输入中对应的关键信息
- 前馈神经网络层：对处理后的信息再做一次非线性变换，同样加上残差连接和层归一化

### 信息流动路径

#### 输入处理

- 每个输入单先转换为向量（词嵌入），在加上位置编码，以保留单词在序列中的位置信息

#### 编码器中的信息流动

- 初始嵌入进入第一层编码器，通过多头自注意力机制，各个单词在表达上会相互 "交流"，捕捉全局信息
- 经由前馈神经网络后，每层都会输出一个新的表示，这个表示包含了输入中各个单词之间的依赖关系
- 经过 N 层堆叠后，编码器的输出就成为一个 "记忆"，包括了整个输入句子的综合信息

#### 编码器 - 解码器的信息交互

- 在解码器中，每个时刻生成下一个单词时，除了利用自己之前已经生成的单词（通过自注意力捕捉上下文信息），还会利用来自编码器的"记忆"信息，通过编码器 - 解码器注意力层获取输入对应的关键信息

#### 解码器生成输出

- 最后经过线性变换和 Softmax 层，解码器输出概率分布，选择最合适的词作为输出，完成整个生成过程

## Transformer 关键技术原理

### 位置编码

```
由于 Transformer 没有 RNN 那种天然的序列顺序信息，需要通过位置编码来提供每个词的位置
```

![](../pics/pos-embeding.png)

- pos 表示词在序列中的位置
- i 表示编码维度的下标
- d-model 是词向量的维度

```
这种设计使得不同位置的编码在不同频率上呈现周期性变化，从而模型可以通过加减操作获得词与词之间的相对位置信息
```

### Scaled Dot-Product Attention

**注意力机制的核心是计算查询（Query）、键（Key）和值（Value）之间的相似度**

![](../pics/attention-formula.png)

- Q 是查询矩阵，K 是键矩阵，V 是值矩阵
- dk 是键向量的维度，用于缩放（防止点积过大导致梯度消失或饱和）
- 先计算 Q 和 K 的点积，再除以 dk 的平方根后做 softmax 得到注意力分布

### Multi-head Attention

为了让模型能够从不同的子空间中捕捉信息，Transformer 使用了多头注意力机制。公式为：

![](../pics/multi-head-atten.png)

其中每个头的计算为：

![](../pics/single-head.png)

- W 是各自头的投影矩阵

**这种设计允许模型同时从多个角度（子空间）对信息进行捕捉和融合**

### 前馈神经网络（Feed-Forward Network, FFN）

每个编码器和解码器层中都有一个位置-wise 前馈神经网络，对每个位置的表示单独进行非线性变换：
**FFN(x) = max(0, xW1 + b1)W2 + b2**

其中：
- W1 和 W2 是权重矩阵，b1 和 b2 是偏置项
- 这里 max(0, ) 表示 ReLU 激活函数

### 残差连接和层归一化

为了解决深层网络中帝都消失的问题，并加快收敛速度，每个子层后面都使用残差连接，并接着做层归一化：
**Output = LayerNorm(x + Sublayer(x))**

*这样可以使信息在网络中更平稳地流动，同时让每一层学习到与输入的差异*

### 解码器中的额外注意力层

解码器除自注意力和前馈网络外，还增加了一个编码器 - 解码器注意力层，其计算方法与标准注意力类似：

- 查询来自解码器当前层
- 键和值则是编码器的输出（即输入信息的"记忆"）

这种设计上让解码器在生成每个输入时，即能考虑之间生成的内容，又能参考输入的整体信息，从而生成更准确的结果














**Reference**

- [突破 Transformer: 讲透注意力](https://mp.weixin.qq.com/s?__biz=MzkzOTc1OTc2Ng==&mid=2247485996&idx=1&sn=49498418e7bd181b9dd47fb06a0726f4&chksm=c2ed4bd5f59ac2c3535cebdc3e7b6ba85e6ec9f64325927b3d2ee0a94fcc909b5cfcccdb8550&cur_album_id=3631456386036367362&scene=189#wechat_redirect)
- [彻底搞懂！Transformer 整体架构](https://mp.weixin.qq.com/s/EQoBdXM12yuVxzh8sW-oag)