# DeepSeek 创新

## DeepSeek V3

### 模型架构创新优化

**混合专家（MoE）架构与智能激活策略**

- MoE 拥有 6710 亿参数，但每个 token 仅激活 370 亿参数
- MoE 采用动态冗余策略，在推理和训练过程中实现高效运行
- MoE 架构将模型分解为多个专家网络，每个专家处理特定输入子集，通过路由机制智能分配输入数据，降低计算成本

**多头潜在注意力机制（MLA）**

- 解决 kv-cache 对显存的压力
- 通过低秩压缩 key-value 矩阵，减少对内存占用并提升推理效率
- 通过稀疏注意力机制优化长序列处理，提升文本任务表现

**无辅助损失的负载均衡策略**

- 传统的 MoE 模型依赖辅助损失函数平衡负载，但是增加了训练复杂度且效果有限
- 通过动态路由偏置调整解决负载均衡问题，避免引入辅助损失函数，提高专家利用率和训练稳定性

**多 Token 预测技术（MTP）**

- 传统自回归语言模型每次仅预测一个 token，限制了长文本的生成表现
- MTP 技术每次推理预测多个 token，提高生成效率和连贯性，并结合投机算法加速推理

### 训练优化

**高效优化**

- V3 在 14.8 万亿高质量 tokens 上预训练，优化预训语料库，增加数学和编程样本比例，扩展多语言覆盖范围
- 开发 FP8 混合精度训练架构，突破跨节点 MoE 训练通信瓶颈，提升训练效率，降低成本

**知识蒸馏和性能提升**

- 后训练阶段引入知识蒸馏方法，将 DeepSeek R1 的推理能力转移到标准大语言模型中
- 通过生成高质量训练样本，整合 R1 的验证和反思机制，显著提升推理能力

**训练过程稳定性**

- 未出现不可恢复的损失峰值，无需回滚操作
- 稳定性得益于创新框架设计和优化策略

### 推理和部署优化

**预填充和解码分离**

- 采用预填充和解码分离策略：提升系统吞吐量，较少解码延时，优化资源利用
- 冗余专家部署和动态路由策略：推理阶段引入冗余专家部署，确保 GPU 处理平衡，同时探索动态冗余策略，优化推理效率

## DeepSeek R1

### 纯强化学习训练

- R1-Zero 版本无需 SFT 数据，仅通过强化学习实现推理能力自主进化，减少模型参数量，且将强化学习能力泛化到无明确奖励信号的领域

### 组相对策略优化算法（GRPO）
- 简化训练复杂度，提升效率，防止 reward hacking

### 多阶段训练策略

- 解决强化学习训练的问题，采用 "冷启动 + 多阶段 RL" 策略
- 冷启动阶段微调基础模型，规范输出格式
- 后续通过两阶段学习优化性能，包括推理导向 RL 和通用对齐 RL

### 长链推理技术

- 思维链长度可达数万字，通过多步骤逻辑推理解决问题，提升复杂任务效率




**Reference**

- [悟透 DeepSeek 创新](https://mp.weixin.qq.com/s/IliUhZOTwyvs2_rQV0Qe8Q)
