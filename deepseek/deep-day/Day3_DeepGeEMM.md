# Day3_DeepGEMM

## 矩阵乘法

- 大模型的每一层神经网络本质都是矩阵变换，不管是输入的是文本还是图像，都是通过权重矩阵变换来实现特征提取，比如大家熟知的 Transformer，自注意力机制就是通过 Query,Key, Value 三个矩阵的乘法完成语义关联计算
- 矩阵乘法占了运行时长的 45% - 60%

`
直白的说，千亿参数规模的模型，90% 以上的计算量来自矩阵乘法。所以矩阵乘法的效率直接决定了模型推理和训练速度。
`

### 面临问题

- 模型结构各异导致矩阵大小差异性非常大，不同的长度序列具有不同的 shape、不同的 batch size，实际矩阵乘法有数百个形状
- 矩阵太大，没办法一次性饭放入到寄存器或内存缓存

`
GPU/TPU 的架构设计，都是高度针对矩阵乘法去优化的，以 NVDIA GPU 为例，其张量核心（Tensor Core）就是专门加速矩阵累加运算的，还有专门的 GPU 加速库 cuBLAS。
`

## DeepGEMM

### 特点

- 核心代码只有 300 行左右，最高可达 1358 TFLOPS
- 仅支持英伟达最新的 Hopper 架构的显卡 H800，不兼容老型号
- JIT（Just-In-Time）编译，在运行时会自动生成需要的代码，不需要配置环境，能快速部署

### 两级累加

`
存在的问题：大模型一般用 FP8 量化瘦身，目的是提速，虽然提取速度快，存储空间小，但是低精度带来误差累积，大模型中的矩阵乘法可能涉及数百万词乘法，误差会被指数级放大。
`

- 先做高精度乘法和累加，当高精度累加结果超过 FP8 范围时，再转回 FP8 存储，实现在速度和精度之间的平衡
- 不仅支持普通精度乘法，还支持混合专家模型 MoE

`
简单地说，普通的 AI 通常是稠密矩阵，MoE 因为不是全部激活，所以还会有稀疏矩阵，一般会分组计算，因此 MoE 中的矩阵乘法更加复杂
`

### MoE 中的 GEMM 场景

**Contiguous Layout**

- 适合训练或推理时把不同的 Expert 的 Token 数据按行连续拼接，有利于用 DeepGeMM 的分组功能一次性进行多组运算

**Masked Layout**

- 适合推理阶段，当 Export 间的 Token 数大多并不均匀或实时变化时，可通过一个 mask 来只计算有效行，从而减少无用的算力浪费

**Reference**

- [300 行代码的矩阵乘法把 H800 榨到冒青烟，一文告诉你 DeepGEMM 到底牛在哪儿](https://mp.weixin.qq.com/s/C-sMu9Yo5xpzuRyEo1TVlA)
- [GitHub DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)