# Noun Explanation

## Bert

**Bidirectional Encoder Representations from Transformers**

### 框架

- 基于 Transformer 编码器部分
- 采用双向编码方式（能够同时考虑上下文的前后信息，从而生成更丰富的词表示）
- 能够同时考虑上下文的前后信息，从而生成更丰富的词表示

### 训练方式

- 主要用于掩码语言模型（masked Language Model, MLM）和下一句的预测（Next Sentence Prediction, NSP）作为训练任务
- MLM 通过随机掩盖输入序列中的一些词来训练模型，从而预测被掩盖的词
- BSP 用于判断两个句子是否连续

### 上下文的理解能力

- 双向特性，在理解整个句子或段落时表现出色，适合需要深入理解文本内容的任务，如情感分析、命名实体识别等

### 应用领域

- 广泛应用于分类、问答系统和句子关系判断等任务


## Gpt

**Generative Pre-trained Transformer**

### 框架

- 基于 Transformer 的解码器部分
- 采用自回归模型（从左到右生成文本，只能依赖于已生成的上下文来预测下一个词）

### 训练方式

- Gpt 使用自回归模型进行训练，专注于预测序列中的下一个词
- 通过在大规模无监督语料上进行训练，逐步生成文本

### 上下文的理解能力

- Gpt 作为单向模型，在生成文本时只能依赖已生成的文本，在处理需要全面理解的任务时可能表现不足
- 文本生成能力较强，适用于对话生成和内容创作等场景

### 应用领域

- 文本生成、机器翻译和对话系统

## BPE

**Byte Pair Encoding**

### 框架

- 一种基于统计的分词算法，最初用于数据压缩
- 通过迭代地合并最频繁的字符对来构建词汇表，减少词汇表的大小
- 减少词汇表大小，保持较高的词汇覆盖率，提高了模型的泛化能力

### 训练方式
- 初始化：将文本中的每个单词分解为字符序列
- 迭代合并：统计所有字符对的频率，选择出频率最高的字符对进行合并
- 跟新词汇表：重复合并过程，直到达到预订的词汇表大小或不再有频繁的字符对

### 应用领域

- 广泛应用于自然语言处理的任务中，如机器翻译、文本生成和语言模型训练

## SFT

**Supervised Fine-Tuning**
- 一种在深度学习和自然语言处理领域中常用的技术
- 通过对预训练模型进行进一步的训练，以提高模型在特定任务上的表现
- 主要用于在预训练模型的基础上，通过有监督的标注数据进一步调整模型参数，使其适应特定任务或领域的需求

### SFT 是什么

- 定义：SFT 是模型预训练的第二阶段（预训练 -> 微调 -> 其它优化）
- 输入数据：使用高质量的标注数据（如问答对、指令-回复对、任务示例），通常由人工标注或经过筛选生成
- 目标：调整预训练模型的参数，使其输出更符合特定任务的要求（如：对话、摘要、分类等）

### SFT 在大模型训练中的作用

#### 任务适配性提升

- 预训练模型（如 GPT、LLaMA） 通过海量无监督数据学习通用语言模型，但缺乏对具体任务的针对性
- SFT 通过标注数据教会模型如何遵循特定指令或格式（如代码生成、回答问题、客服对话），显著提升任务表现

#### 控制输出风格和质量

- 调整输出风格：例如让模型生成更正式、简洁或口语化的内容
- 减少有害/偏见输出：通过标注数据引导模型规避错误、偏见或不安全的内容

#### 解决预训练模型的局限性

- 预训练模型可能对低频任务（如专业领域术语）表现不佳，SFT 通过针对性数据增强其能力
- 修正预训练中未对齐的人类偏好或任务需求的行为（如冗余回复、答非所问）

### SFT 的典型流程

- 预训练模型：使用无监督数据（如互联网文本）训练基础模型（如 GPT-3）
- 构建监督数据集：收集任务相关的标注数据（如人工编写的问答对、指令回复对）
- 微调训练：在标注数据继续训练，调整模型参数（通常学习率较低、避免覆盖预训练的知识）
- 评估和迭代：验证模型在目标任务上的表现，调整数据和训练策略

### 示例应用场景

- 对话系统：用用户-助手对话数据微调，使模型生成更自然的交互回复
- 代码生成：用代码注释-代码对数据训练模型生成符合需求的代码片段
- 内容安全：通过标注的 "安全回复" 数据减少模型的有害输出

## RLHF

**Reinforcement Learning from Human Feeback 基于人类反馈的强化学习** 

### 定义

- 定义：RLHF 是一种结合强化学习和人类反馈的学习方法，通过人类对模型输出的评分或排序，训练模型生成更符合期望的文本
- 适用阶段：通常在大模型完成 预训练（Pretraing）和 监督微调（SFT） 之后使用，属于对齐（Alignment） 阶段的核心技术
- 核心思想：人类无法直接标注所有可能的输出，但可以通过对少量样本的反馈（好坏排序）训练一个奖励模型（Reward Model），再使用该模型代替人类指导强化学习

### RLHF 在 LLM 中的作用

#### 对齐人类偏好（Alignment）

- 问题：预训练或 SFT 后的模型可能生成不友好、有害或偏离用户意图的内容
- RLHF方案： 通过人类反馈定义 "好输出" 的标准（如安全性、有用性、真实性），训练模型对齐这些标准
- 示例：让模型避免生成歧视性言论或虚假信息

#### 提升生成内容的质量

- 优化主观性指标：如流畅性、逻辑性、创造性等难以通过监督学习直接优化的目标
- 示例：让模型生成更生动有趣的故事，或更符合特定场景的对话回复

#### 解决 SFT 的局限性

- SFT 依赖高质量的标注数据，但标注成本高且难以覆盖所有场景
- RLHF 通过少量反馈数据泛化到更广泛的情况，且能处理模糊或多目标的任务（如："既要简洁又要全面"）

#### 控制输出的安全与伦理

- 通过人类反馈明确禁止模型生成暴力、违法或违背道德的内容
- 示例：ChatGPT 通过 RLHF 大幅减少有害回复的概率

#### 数据高效性

- 相比需要海量标注数据的监督学习，RLHF 只需人类多少量输出进行评分或排序，即可训练奖励模型指导优化

### RLHF 的工作流程

**预训练 & SFT**

- 使用无监督数据训练基础模型（如 GPT-3），再通过监督微调（SFT） 初步对齐任务需求

**收集人类反馈**

- 让人类标注员对模型生成的多个输出进行评分或排序（如对同一个问题生成 4 个答案，标注员按照质量排序）

**训练奖励模型**

- 用人类反馈数据训练一个 "奖励模型"，使其能够预测人类对任意输出的偏好程度

**强化学习微调（PRO 算法）**

- 将预训练模型作为策略，奖励模型作为环境，使用强化学习算法（PRO） 优化模型，最大化累积奖励
- 关键点：需约束模型不要过度偏离原始预训练分布（防止 "奖励黑客" 或市区通用性）

### RLHF 的示例应用

- ChatGPT: 通过 RLHF 大幅提升对话的友好性、安全性和实用性
- Claude: 利用 RLHF 避免模型生成政治敏感或偏激的内容
- GPT-4: 结合 RLHF 优化复杂任务（如代码生成、多轮推理）的输出质量

## SFT Vs RLHF

### 核心目标

| SFT | RLHF | 
| :----: | :----: |
| 让模型学会执行特定任务（如问答、翻译）通过标注数据明确 "正确的输出"是什么 | 让模型对齐人类偏好（如安全性、有用性），通过反馈优化"主观质量" 或复杂目标（如 "简洁且全面"） |
| 示例：教模型按格式生成代码 | 示例：让模型避免生成有害内容，或生成更自然的对话回复 |

### 数据使用方式不同

| SFT | RLHF | 
| :----: | :----: |
| 依赖高质量标注的输入-输出对（如问题-答案、指令-回复） | 依赖人类对模型输出的反馈（如对多个输出的排序、评分或偏好标注） |
| 数据特点：明确、具体、任务相关 | 数据特点：主观、抽象、反映人类偏好 |
| 数据量：通常需要大量标注数据 | 数据量：反馈数据量较少，但需要覆盖多样性场景 |

### 训练方式不同

| SFT | RLHF | 
| :----: | :----: |
| 使用监督学习（Supervised Learning），直接通过交叉熵损失优化模型输出与标注答案的一致性 | 使用强化学习（Reinforcement Leaning），通过奖励模型（Reward Model）间接优化模型输出 |
| 训练方式：单阶段，直接调整模型参数 | 训练方式：多阶段（需要先训练奖励模型，再优化策略模型） |
| 优化目标：最小化预测误差 | 优化目标：最大化累积奖励（人类偏好得分） |

### 适用场景差异

| SFT | RLHF | 
| :----: | :----: |
| 任务明确，输出标准清晰：如分类、翻译、结构化生成 | 任务复杂，目标主观或需多目标平衡：如对话安全性、创造性写作、伦理对齐 |
| 示例场景：生成符合格式的 JSON 数据 | 示例场景：避免生成政治敏感内容 |

## PPO

**Proximal Policy Optimization（近端策略优化）**

- 是强化学习 （Reinforcement Learning）中一种广泛使用的策略优化算法
- 核心目标：是让智能体（Agent） 在复杂环境中稳定高效地学习策略，同时避免传统方法中训练不稳定的问题

### PPO 是什么？

- 策略梯度方法 (Policy Gradient) 的改进版本，由于 OpenAI 在 2017 年提出。通过限制策略更新的幅度（即 "近端" 约束），确保每次更新不会偏离当前策略太远，从而提升训练的稳定性

## GRPO 

**Group Relative Policy Optimization**

- 通过组内相对奖励来优化模型的策略，而不依赖传统的批评模型（critic model）

### GRPO 的核心原理

#### 相对奖励机制

- 通过从当前策略中采样一组输出，并计算这些输出的相对表现来进行优化
- 避免对每个输出进行单独评估的复杂性，简化了训练过程

#### 工作流程

- 输出采样：对于每个任务，GRPO 生成多个可能的答案或推理步骤
- 奖励计算：对这些输出进行评分，计算每个输出的奖励，并根据相对表现确定其优势
- 策略优化：根据相对优势调整策略，使得表现比较好的输出更有可能被生成，而表现比较差的输出被抑制

#### 优势和特点

- 简化训练：由于不需要评判模型，GRPO 显著降低了计算成本和复杂度
- 高效优化：通过组内相对奖励，GRPO 能够在复杂任务中提高模型性能

### 在 LLM 中的应用

`
GRPO 特别适合用于推理任务，如数学推理和代码生成等。在这些任务中，模型需要在多个步骤中逐步生成答案，GRPO 通过优化这些步骤的生成策略，提高了模型在推理过程中的准确性和效率
`

## MoE

**Mixture of Experts**

- 一种先进的神经网络架构
- 通过将多个专门化的子模型（即"专家"）组合在一起，以提高模型的效率和性能

### 核心概念

- 专家（Experts）：每个专家是一个独立的子网络，专注于特定类型的任务和数据处理；例如：在自然语言处理任务中，一个专家可能专注于语法分析，另一个专注于情感分析
- 门控系统（Gating Network）：该组件负责根据输入数据动态选择哪些专家参与处理，这种选择机制使得只有少数专家被激活，从而提高计算效率
- 稀疏激活（Sparse Activation）：在每次计算中，只有一小部分专家被激活，这降低了每次推理所需的计算负载，使得模型能够在较低的资源消耗下处理复杂任务

## RoPE

**Rotary Position Embedding（旋转位置编码）**

- 一种基于 Transformer 模型的位置编码方法
- 将位置信息有效集成到自注意力机制中，以提升模型性能

### 核心原理

- 位置信息编码：通过将位置信息编码为旋转向量，将其与词嵌入向量相加，从而增强模型对位置信息的理解
- 旋转角度计算：根据位置信息计算旋转角度，使得不同位置的词嵌入向量在不同角度上有差异

### 作用

- 集成相对位置信息：能够将显式的相对位置依赖性无缝集成到自注意机制中，使得模型在处理序列时能够更好地捕捉到 token 之间的关系
- 良好的外推性：与传统的绝对位置编码不同，RoPE 允许在推理时处理超过训练长度的输入
- 降低计算复杂度：与线性注意力机制兼容，使得在处理长文本时，计算复杂度 
- 增强模型性能：通过结合绝对和相对位置编码的特点，提升 Transformer 架构在多种任务中的表现，包括自然语言处理、文本生成等领域

## LoRA

**Low-Rank Adaptation**

- 一种用于微调大型预训练模型的技术
- 通过引入低秩矩阵来显著减少微调所需的参数数量和计算资源，同时保持模型的性能

### 核心原理
- 低秩矩阵分解：将一个大的可学习权重矩阵分解为两个低秩矩阵，通过添加两个可学习的低秩矩阵 A 和 B 进行微调：
![lora](../pics/lora.png)
其中：a 是缩放因子，控制 LoRA 部分对输出的影响
- 冻结预训练权重：在微调过程中，LoRA 保持预训练的原始权重不变，仅更新低秩矩阵 A 和 B。这种方法显著减少了需要调整的参数数量，降低了计算成本和内存需求

### 在大语言模型中的作用

- 高效微调：LoRA 能够在对象 GPT、Bert 等大语言模型进行微调时，仅调整 0.1% 的参数，同时保持与全量微调相近的性能。这使得 LoRA 特别适合在计算资源有效的环境中使用
- 跨领域迁移学习：LoRA 可以快速适应新的领域数据，例如从自然语言理解迁移到医学文本的分析，而无需对整个预训练模型重新微调。这种灵活性对于快速响应不同任务需求至关重要
- 低资源设备上的部署：由于 LoRA 仅需存储和加载小规模的低秩矩阵，使得在资源受限的设备上，如边缘计算或移动设备，也能够实现高效的微调和推理
- 多任务学习: LoRA 允许在同一模型中处理多个任务，通过调整不同的低秩矩阵来适应各种不同任务需求，从而提高模型的通用性和适应能力

## MHA Vs MLA

### MHA

**Multi-Head Attention**

- 将注意力机制扩展到多个 "头" 的方法
- 通过过个独立的注意力计算来提取输入数据的多样特征
- 每个头可以关注输入的不同部分，从而增强模型的表达能力

#### MHA 的原理

- 输入：MHA 接收三个变量：查询（Query）、键盘（Key）和值（Value）
- 计算过程：
  - 每个头独立计算查询、键和值的点积，得到注意力分数
  - 对注意力分数进行归一化，得到注意力权重
  - 将注意力权重与值相乘，得到加权和，作为每个头的输出
  - 将所有头的输出拼接起来，得到最终的多头注意力输出
  - 通过线性变换将输出映射到所需的维度

#### 应用

 - MHA 在 Transformer 模型中起着关键作用，广泛应用于机器翻译、文本生成等任务，能够捕捉长距离依赖关系

 ### MLA

**Multi-Head Latent Attention**

#### 定义

- 一种新兴的注意力机制，旨在优化传统 MHA 的性能
- 通过引入潜在空间和动态特征选择来改变注意力计算，提高模型效率和灵活性

#### 原理

- 动态特征选择：MLA 采用灵活的方法来选择关注的特征，通过潜在空间中的投影矩阵来替代固定的分割和重复机制
- 高效计算：通过较少需要处理的数据量，提高计算效率，同时保持模型的性能

#### 应用
- 适用于各种深度学习任务，尤其是在资源受限环境下，如移动设备或嵌入式系统

### 对比

| 特征 | MHA | MLA | 
| :----: | :----: | :----: |
| 基本概念 | 多头注意力机制 | 多头潜在注意力机制 |
| 输入类型 | 查询、键、值 | 查询、键、值 |
| 计算方式 | 独立计算多个头的注意力 | 动态选择与压缩输入信息 |
| 效率 | 计算复杂度较高 | 通过预处理降低计算复杂度 |
| 灵活性 | 固定的头部关注 | 更灵活的特征选择能力 |
| 应用场景 | 广泛应用于 NLP 任务 | 使用于资源受限环境下的任务 |

## CoT

**Chain of Thought**

- 一种用于大语言模型的提示技术
- 旨在提升模型在复杂推理任务中的表现
- 通过引导模型在生成最终答案之间，逐步推导出一系列中间步骤，从而模拟人类的思考过程

### CoT 的作用

- 提升复杂问题解决能力：将复杂问题拆解为多个子问题，使得模型在处理时更具理性和逻辑性，从而提高解决问题的能力
- 增强模型可解释型：通过展示推理过程，CoT 使得模型的决策过程更加透明，用户可以更容易理解模型如何得出结论，并能够识别潜在的错误步骤
- 促进学习和进化：帮助模型更好地理解问题本质，提升学习能力，使得在面对新问题时能够更快适应

### CoT 的示例

**当被问道一个数学问题时，CoT 的推理步骤：**

- 理解问题：首先，模型会解析问题的内容，识别出关键变量和条件
- 分解步骤：模型将复杂问题分解为更简单的子问题。例如：如果问题是计算某个物体的速度，模型可能会首先计算距离和时间
- 逐步推理：对于每个子问题，模型会进行详细的逻辑推导，并解释每一步的原因
- 得出结论：最后，模型整合所有步骤的结果，给出最终的答案

## RL（强化学习）

**Reinforcement Learning**

### 定义

- 一种通过与环境交互来学习最优策略的机器学习方法
- 通过执行动作、观察环境反馈（奖励或惩罚），不断调整自己的策略，以最大化累积奖励
- 与依赖标注数据的监督学习和无标注数据中寻找模式的无监督学习不同，RL 专注于通过试错从直接反馈中学习

### 在大语言模型中的作用

- 优化生成质量：用于微调大语言模型，使其生成的文本更符合人类偏好。例如：通过引入奖励模型，对生成文本进行评分，从而指导模型改进输出质量
- 提高样本效率：大语言模型通常需要大量的数据进行训练，而 RL 能够通过模拟或利用先验知识来提高样本效率，减少对环境交互的需求。这使得模型能够在较少的数据下快速适应新任务
- 多任务学习：通过设计合适的奖励函数， RL 可以促进模型在多个任务之间的泛化能力。例如：在自然语言处理任务中，模型可以同时处理翻译、问答和文本生成等任务
- 人类反馈强化学习（RLHF）：将人类反馈融入到强化学习中的一种方法，通过让人类评估模型的输出并给予奖励，进一步优化模型性能。这种方法在自然语言理解和生成任务中表现尤为突出

## MTP

**Multi-Token Prediction**

- 一种用于大语言模型的训练和推理方法，旨在提高模型效率和性能

### MTP 的作用

- 提高训练效率：通过一次性生成多个 token，MTP 使得模型能够学习更长的 token 依赖关系。这种方法促进模型更好的理解上下文，避免陷入局部决策的学习模式，从而加速模型收敛
- 增强推理速度：在推理阶段, MTP 允许并行预测多个 token，显著提高推理速度。通过减少每次生成时的计算次数，模型能够更快速地响应输入
- 提高泛化能力：MTP 在训练过程中通过多步 token 预测，增强了模型对长距离依赖关系的学习能力。这使得模型能够更好地处理复杂任务，提高其泛化能力
- 优化内存使用：传统的逐个 token 生成方式会频繁范文内存，而 MTP 通过减少这种交互，优化内存使用效率，降低训练和推理过程中的计算瓶颈
- 适应性强：可以灵活应用于不同类型的任务，如文本生成、机器翻译等

## DualPipe

- 一种创新的流水线并行算法，主要用于提高大语言模型训练过程中的效率，尤其是在跨节点的分布式训练场景中
- 通过优化计算和通信的重叠，显著减少了训练时间和资源浪费
- 传统训练信息流水线会产生一些等待时间，有 "流水线气泡"。双重流水线，让一个计算阶段在等待数据传输时可以切换到另一批数据，充分利用空闲时间

## TTS

**Text-to-Speech**

- 将文本转换成语音的深度学习模型

### 原理

- 利用深度学习技术，如卷积神经网络（CNN）、循环神经网络（RNN） 或 Transformer 等，对大量的语言数据和对应的文本进行学习，从而理解文本中的语义信息，并将其转换为相应的语音信号

### 模型的结构

- 文本编码器：负责将输入的文本转换为相应的语义向量，常用的结构包括 RNN 和 Trasformer
- 声学编码器：将语义向量转换为声学特征，如 Mel 频谱，常见结构包括 CNN 和 RNN
- 声学编码器：将声学特征转换为语音波形，常用的解码器包括基于长短时记忆网络（LSTM） 和 Transformer 的解码器

## RNN

**Recurrent Neural Network（循环神经网络）**

### 定义

- 专门处理 "有顺序的数据" 的神经网络，比如一句话、语音、时间序列（比如股票的价格），它的特点是能记住 "前面的信息"，用来影响后面的结果 

### 工作原理
- 输入：RNN 接收一个序列的输入，通常是一个向量序列，每个向量表示序列中的一个元素
- 状态：RNN 内部维护一个 "状态"，用于存储之前的信息
- 计算：对于序列中的每个元素，RNN 执行以下步骤：
  - 计算当前元素与状态的组合，得到新的状态
  - 输出当前元素的预测结果
  - 更新状态，准备处理下一个元素  

## CNN

**Convolutional Neural Network（循环神经网络）**

- 专门处理 "网络状数据" 的神经网络，比如图片、视频
- 用 "局部扫描"  提取关键特征（比如边缘、形状），再组合起来识别整体

### 例子

**如何识别一张图片是不是猫**

- 先扫描图片的局部区域，识别出 "边缘"（比如猫的耳朵的轮廓）
- 再组合边缘，识别出 "形状"（比如猫的耳朵、眼睛）
- 最后综合所有特征，判断这是猫

## LSTM

**Long Short-Term Memory**

- 一种特殊的神经网络，专门用来处理 "有顺序的数据"（比如一句话、 股票的价格走势），它能记住长期信息，解决普通神经网络 "记忆性差" 的问题

### LSTM Vs RNN

- RNN： 像金鱼，只能记住最近几个词，比如 "我爱吃"，可能猜测下一个词是 "饭"
- LSTM: 像人类，能记住更远的上下文，比如 "我从小在安徽萧县长大，所以我爱吃"，这时它会结合 "萧县" 猜测下一个词可能是 "面皮"

### 为什么需要 LSTM

- 问题：普通 RNN 来处理长序列（比如长文章）时，会 "忘记" 开头的内容
- 原因：信息在传递的过程中逐渐消失或爆炸（技术术语： 梯度消失/爆炸）
- LSTM：通过 "门控机制" 决定记住什么，忘记什么，控制信息流

### 三大 "门"

- 遗忘门：决定 "忘记"  哪些旧的信息
- 输入门：决定 "记住" 哪些新信息
- 输出门：决定当前时刻输出什么结果

## UCT

**Upper Confidence Bound for Trees**

- 蒙特卡洛树搜索中的一个核心算法策略
- 用于在决策树搜索过程中平衡 "搜索" 与 "利用"

### 为什么需要 UCT

想象一个多臂老虎机。每个摇臂的赢钱概率未知，需要不断尝试，找到赢钱概率最高的摇臂。但面临一个问题：

- 利用 (Exploitation): 一直拉当前赢钱最多的摇臂（但可能错过其他更优的摇臂）
- 探索 (Exploration): 尝试其他摇臂，收集更多信息（但可能浪费机会）

**UTC 就是为了解决这类 "探索-利用" 而设计的数学方法**

### 核心原理

- 利用项（Exploitation Term）： 反应当前已知的 "平均收益"，鼓励选择表现好的动作
- 探索项目（Exploration Term）：反应该动作的 "探索不足程度"，鼓励选择尚未尝试过的动作

![uct](../pics/uct.png)

![uct-demo](../pics/uct-demo.png)

### UCT 的作用

#### 避免局部最优

- 防止智能体过早锁定当前看似最优的路径（可能不是全局最优）

#### 动态调整策略

- 随着节点访问次数的增加，探索项的权重降低，逐渐偏向利用已知最优路径

#### 提升搜索效率

- 在有限的计算支援下，UCT 能快速聚焦到有潜力的分支，减少无效的搜索路劲

### 局限性

- 计算开销大：需要实时计算 UCT 得分，对实时性要求高的场景并不适用
- 依赖参数 C： 参数 C 需要手动调整，不同任务可能需要不同的调参经验

### UCT Vs 贪婪策略

| 纯贪婪策略 | UCT 策略 |
| :----: | :----: |
| 永远选择当前最优路径 | 平衡探索和利用 |
| 容易陷入局部最优 | 更可能找到全局最优 |
| 适合简单、确定性环境 | 适合复杂、不确定性环境 |


## MCTS (蒙特卡洛搜索树)

**Monte Carlo Tree Search**

- 一种用于复杂决策问题的智能搜索算法，尤其在不确定环境中（如： 棋类游戏、 路径规划）表现优异
- 核心思想：通过随机模拟和树形结构扩展，逐步找到最优决策路径

### 搜索的四个步骤

#### 选择

- 从当前状态（树的根节点）出发，根据现有信息选择子节点
- 通常使用 UCT（Upper Confidence Bound for Tress）公式平衡探索（尝试新路径）和利用（选择已知最优路径）
- 简单理解：优先选择 "胜率高" 的分支，但偶尔也试试没探索过的分支

#### 扩展

- 如果当前节点未被完全探索，扩展一个新的子节点（即尝试一个新动作）
- 例如：在围棋中，如果某个落子位置从未被模拟过，就将其加入决策树

#### 模拟（Simulation）

- 从新扩展的节点出发，随机模拟后续动作，直到得到一个结果

#### 回溯

- 将模拟结果反向更新到路径上的所有节点，调整它们的统计数据

### 作用

#### 处理高复杂度决策

- 适用于状态空间巨大的问题（如围棋有 10 的 170 次方种可能的状态），传统穷举法无法解决，而 MCTS 能通过模拟和剪枝高效找到最优近似解

#### 无需预训练模型

- 不同于依赖神经网络的深度学习，MCTS 可以直接从零开始探索环境，适合规则明确但状态复杂的问题

#### 动态适应环境

- 随着模拟次数增加，树的结构会动态调整，逐渐聚焦到更有潜力的分支

### 应用场景

- 棋类游戏：如 AlphaGo 核心算法之一 MCTS，结合神经网络评估棋局
- 路径规划：机器人或自动驾驶中寻找最优路径

## Beam Search

- 一种用于序列生成任务的启发式搜索算法
- 广泛应用于机器翻译、语音识别、文本生成等领域
- 目标是在有限计算资源下，找到全局最优的序列，避免贪婪搜索的短视问题

### 核心思想

- 贪婪搜索：每一步只选当前看似最优的路径，可能是最终走到死胡同
- 穷举搜索：尝试所有路径，计算量爆炸，完全不现实
- 束搜索：折中方案 - 每一步只保留最有可能的几条路径（称为束宽），逐步淘汰较差的路径

## Process-Based Reward Model（基于过程的模型奖励）

- 一种强化学习中设计奖励机制的方法
- 核心思想是根据任务执行过程中的阶段性表现动态提供奖励，而非仅在任务结束时给出单一结果奖励

### 为什么需要基于过程的奖励模型

**在传统强化学习中，奖励通常是稀疏的（Sparse Reward）：**

- 围棋：仅在游戏结束时给出赢/输的奖励
- 机器人任务：仅在机器人成功抓取物体或摔倒时给出奖励

**稀疏奖励的问题：**

- 训练效率低：智能体需要大量随机尝试才能偶然获得奖励，学习速度慢
- 难以学习中间步骤：缺乏对过程行为的反馈，可能导致策略不稳定

`
而基于过程的奖励模型通过分解任务为多个子目标，在每个关键步骤中提供奖励信号，显著改善这些问题
`

### 核心原理

#### 过程分解

`
将复杂任务拆分为多个子任务，并为每个子任务设计奖励
`

**例如: 训练机器人 "走到目标位置" 可分解为：**

- 保持平衡 -> 奖励 1
- 迈出一步 -> 奖励 2
- 方向正确 -> 奖励 3
- 到达目标 -> 最终奖励

#### 动态奖励分配

`
根据当前状态或行为实时计算奖励值，而非依赖最终结果
`

**常用方法：**

- 人工设计：专家手动定义子目标奖励规则（如距离目标越近，奖励越高）
- 自动学习：通过逆学习（Inverse RL）或模仿学习（Imitation Learning） 从专家数据中推断过程奖励

#### 奖励函数形式

**奖励函数可能包含多个维度**

- 进度奖励（如移动距离）
- 安全惩罚（如碰撞扣分）
- 权重参数


## 蒸馏模型 (distillation)

`
蒸馏模型是一种模型压缩技术，核心思想是让一个小模型去模仿一个大模型的行为，最终让小模型达到接近大模型的效果，但体积更小，速度更快
`

### 蒸馏模型如何工作？

#### 教师模型：已经训练好的复杂大模型，性能强但体积大、推理慢

#### 学生模型：结构更简单的小模型（例如 TinyBERT、 MobileNet 等），目标是模仿教师模型的输出

#### 蒸馏过程

- 教师模型对输入数据生成软标签（Soft Labels）（例如分类概率分布，如 猫： 0.8， 狗： 0.15， 老虎： 0.05），而不仅是最终答案（硬标签： 如 猫）
- 学习模型学习的目标是同时匹配硬标签（真实答案）和软标签（教师模型输出）
- 通过这种方式，学生模型不仅能学到 "答案是什么"，还能学到教师模型对问题的 "思考方式" (例如不同类别间的相似性)

#### 损失函数与优化

为了确保学生模型能够尽可能接近教师模型的输出，需要使用一个损失函数来衡量学生模型和教师模型之间的差异。损失函数通常包含两个部分：

- KL 散度：计算两个概率分布之间的差异，衡量的是学生模型的输出和教师模型的输出之间的 "距离"，木的是让学生模型尽可能模仿教师模型的输出
- 交叉熵损失：用来衡量学生模型预测的标签和真实标签之间的差距，通常用于分类任务中，表示预测值与真实值之间的误差

## Corss Entropy Loss（交叉损失函数）

`
交叉损失函数（Cross Entropy Loss）是机器学习中常用于分类任务的损失函数，用于衡量模型预测的概率分布与真实标签分布之间的差异
`

### 核心概念

**信息论基础**

- 熵（Entropy）：表示一个事件的不确定性，熵越大，不确定性越高
- 交叉熵（Cross Entropy）：衡量两个概率分布之间的差异，用于衡量模型预测与真实标签之间的距离

**数学公式**

- 对于二分类问题

![cross-entropy](../pics/cross-entropy.png)

- 对于多分类问题（如 C 个类别）

![cross-entropy](../pics/cross-entropy-2.png)

### 实际例子

**二分类**

- 真实标签 y = 1，模型预测概率 p = 0.9
- 单样本损失 L = - 1 * log(0.9) = 0.105

**多分类**

- 真实标签为第三类（one-hot 编码 [0, 0, 1]），预测概率为 [0.1, 0.2, 0.7]
- 单样本损失： L = -log(0.7) = 0.356

### 常见应用场景

**分类任务**

- 图像分类、文本分类、垃圾邮件检测等
- 通常搭配 Softmax（多分类）或 Sigmoid（二分类）使用

## MSE

**Mean Squared Error (均方差)**

- 一种常用的评估指标
- 用于衡量模型预测值与真实值之间的差异
- 通过计算预测误差的平方的平均值来量化模型和预测性能

### 定义

MSE 公式如下：

![mse](../pics/mse.png)

其中：

- n 是样本数量
- yi 是第 i 个样本的真实值
- ^yi 是第 i 个样本的预测值

### 作用

- 模型评估：回归问题中最常见的损失函数之一，能够有效评估模型的预测准确性。值越小，表示模型对数据的拟合程度越好
- 优化目标：通过最小化 MSE 来优化模型参数，帮忙模型逐步提高其预测能力

## KL 散度

**Kullback-Leibler Divergence**

- 一种用于衡量两个概率分布之间差异的非对称性度量，也称为**相对熵**

### 定义

KL 散度公式如下：

![kl](../pics/kl.png)

其中：
- P 是真实分布
- Q 是预测分布
- Xi 是随机变量的取值

**对于连续随机变量，KL 散度的定义：**

![kl](../pics/kl-2.png)

## 激活函数

`
激活函数是神经网络中的关键组件，负责引入非线性特性，使得模型能够学习和表示复杂的模式
`

### 作用

**引入非线性特性**

- 使得神经网络能够捕捉数据中的非线性关系
- 如果没有激活函数，网络的每一层将仅进行线性变换，使得整个网络退化为单层线性模型，无法处理复杂任务

**控制神经元的激活**

- 激活函数决定哪些神经元被激活，从而影响信息流
- 例：RelU (Rectified Linear Unit) 函数，将所有负数设为 0，正数保持不变，这意味着输入较小的神经元将不会对输出产生影响

**影响模型收敛的速度和效果**

- 不同的激活函数对模型训练和效果有直接影响
- Sigmod 和 Tanh 函数在深层网络中可能导致梯度消失
- ReLU 及其变种能够保持有效的梯度，加快收敛速度

### 常见的激活函数
**Sigmoid 函数**

- 将模型输出转换为概率分布，使得每个类别的概率总和为 1
- 通常用于神经网络的最后一层，以处理多类分类任务，如图像分类或文本生成
- 公式：
  ![softmax](../pics/softmax.png)
- 示例：在图像分类任务中，如果模型输出为 [2.0, 1.0, 0.1]，经过 Softmax 处理后可能得到 [0.73, 0.25, 0.02]，表示第一类的预测概率最高

**Sigmoid**

- 用于二分类问题，将输入值映射到 (0, 1) 之间
- 表达式：
  ![sigmoid](../pics/sigmoid.png)
- 将模型输出压缩到一个概率值，如果输出大于 0.5，则预测为正类；否则预测为负类
- 示例：情感分析任务中，模型输出 [0.8, -1.2, 0.5]，经过处理后得到 [0.69, 0.23, 0.62]（可理解为每个标签存在的概率）

## FFN

**Feed-Forward Network（前馈神经网络）**

### 概览

- FFN 层是 Transformer 模型中的 "信息加工厂"
- 对每个词的表示进行非线性变换，增强模型对复杂模式的表达能力
- 自注意力（Self-Attention）层负责捕捉词之间的关系，FFN 层负责深化每个词自身的特征表示

### 结构

**FFN 层是一个简单的神经网络，通常包含两个线性变换层和一个激活函数**

`
输入 -> 线性层（扩大维度） -> 激活函数（如 ReLU、 GeLU） -> 线性层（缩小回原维度）-> 输出
`

**具体步骤：**

- 输入：来自自注意力层的词向量（例如维度是 1024）
- 第一次线性变换（扩大维度）：
  将输入维度扩大（例如 1024 -> 4096），这一步可以看作 "提取更多潜在特征"
- 激活函数：
  使用非线性函数（如 GeLU），目的是引入非线性，让模型能学习复杂模式
- 第二次线性变换（缩小维度）：
  将维度缩小回原始大小（如 4096 -> 1024），这一步是 "精炼特征并过滤噪声"

### 为什么需要 FFN 层

#### 补充自注意力层的不足

- 自注意力层：擅长捕捉词与词之间的全局依赖，但本质是线程加权组合
- FFN 层：通过非线性变换，让模型学习更复杂的特征组合（例如："猫" 在不同语境下的隐含语义）

#### 增强模型表达能力

- 如果只有自注意力层，模型相当于多个线性变换的堆叠，表达能力有限
- FFN 层通过非线性激活函数（如 GeLU）和维度扩展，让模型能拟合更复杂的函数

#### 位置独立的特征变换

- 自注意力层输出的每个词向量已经包含上下文信息，FFN 层会独立处理每个词向量，进一步细化其特征

### FFN 层在 Transformer 中的位置

`
输入 -> 自注意力层 -> 残差连接（Residual Connection） -> 层归一化 -> FFN 层 -> 残差连接（Residual Connection） + 层归一化（LayerNorm） —> 输出
`

### FFN 层的参数规模

**FFN 层是 LLM 中参数量最大的部分**

`
扩大维度可以容纳更多知识，但也会增加计算量。这是模型效果与效率的权衡
`

- 在 GPT-3 中，FFN 层的维度是输入维度的 4 倍（如输入 1024 维 -> FFN 中间层 4096 维）
- 一个 1750 亿参数的 GPT-3 模型，约 70% 的参数集中在 FFN 层

### 实际场景

#### 场景：模型理解歧义词

**句子1：苹果发布了新手机**
**句子2：他吃了一个苹果**

- 自注意力层：识别 苹果 与 发布、手机或吃的关系
- FFN 层：将 苹果 的向量转换为不同语义空间
   - 句子 1 中的苹果 -> 靠近 公司 科技 的向量
   - 句子 2 中的苹果 -> 靠近 水果 食物 的向量

### FFN 的变体

**激活函数选择**

- 原始 Transformer 使用 ReLU，现代 LLM（如 GPT）改用 GELU，梯度更平滑

**参数共享**

- 部分模型在多层之间共享 FFN 参数以减少计算量

**稀疏 FFN**

- 使用多 FFN 专家，每次只激活其中一个（MoE 架构）

## Redisual Connection

**Residual Connection (残差连接)**

**核心思想：**

`
通过引入 "跳跃连接（Skip Connection）"，将输入直接传递到后续层的输出端，形成 输入与变换后的输出 的叠加
`

### 基本结构

在 Transformer 的每个子层（如自注意力层或前馈神经网络层）中，残差连接的结构如下：

  ![residual-connection](../pics/residual-connection.png)

或

  ![residual-connection](../pics/residual-connection-2.png)

其中：

- x：输入向量
- Sublayer(x)：子层（如自注意力或 FFN ）对输入的处理结果
- LayerNorm：层归一化操作

### 核心作用

#### 缓解梯度消失（Gradient Vanishing）

- 问题：在深层网络中，反向传播时梯度可能逐层衰减，导致底层参数难以更新
- 解决：允许梯度直接通过跳跃连接回传到底层，绕过非线性变换的衰减效应，确保深层网络的稳定训练

#### 简化深层网络训练

- 残差学习：网络无需直接学习复杂的映射 H(x)，而是学习残差 F(x) = H(x) - x。当最优映射接近恒等函数时，学习 F(x) -> 0 比直接学习 H(x) -> x 更简单
- 跳过冗余层：原始输入特征可直接传递到深层，避免因多次非线性变换导致的信息损失

#### 增强特征整合

- 多级特征融合：通过逐层叠加残差，模型能够整合浅层的局部特征与深层的全局语义信息（例如底层捕捉预发，高层捕捉语义），提升表达能力
- 信息直通通道：原始输入特征可直接传递到深层，避免因多次非线性变换导致的信息损失

### 在 LLM 的应用

**以 Transformer 模型为例，每个子层的处理流程：**

- 自注意力层：计算词与词之间的依赖关系
- 残差连接：将自注意力层的输出与原始输入相加
- 层归一化：稳定数值分布，加速收敛
- 前馈神经网络（FFN）：进一步非线性变换
- 再次残差连接与归一化：重复步骤 2-3

## Gradient Desecent 

**Gradient Desecent （梯度下降）**

- 通过调整模型参数，逐步降低模型预测结果与真实数据之间的误差
- 找到让模型表现最好的参数方向，并沿着这个方向不断优化

### 具体作用

#### 优化模型参数

- 目标：调整 LLM 的权重参数（如 Transformer 中的注意力权重、FFN 层参数），使得模型输出更接近真实文本
- 数学表达式：

![gradient-desecent](../pics/gradient-descent.png)

#### 最小化损失函数

- 损失函数：衡量模型预测与真实数据的差异（例如交叉熵损失）
  - 示例：LLM 预测下一个词是 猫 的概率为 0.3，真实概率为 1.0，则损失值较高
- 梯度下降的任务：找到使损失值最小的参数组合

#### 支持大规模训练

- 分布式计算：LLM 参数量极大，梯度下降可通过并行计算加速（如数据并行、模型并行）
- 内存优化：结合梯度累积（Gradient Accumulation）等技术，解决显存不足的问题

### 实现步骤

- 前向传播：输入句子 "我爱吃_", 模型预测下一个词的概率分簇（如 苹果：0.6, 香蕉：0.4）
- 计算损失：若真实词是 苹果，计算预测误差（如交叉熵损失）
- 反向传播：从损失值出发，通过链式法则计算每个参数的梯度
- 参数更新：沿梯度方向调整参数，江辞下一次预测的损失
- 重复迭代：遍历海量数据（如数万亿 token），直到模型收敛

### 梯度下降的变体

**随机梯度下降（SGD）**

- 每次用一个样本计算梯度，更新参数

**小批量梯度下降（Mini-batch Gradient Descent）**

- 每次使用一小批样本计算梯度，更新参数

**自适应优化器（如 Adam）**

## AIME

**American Invitational Mathematics Examination（美国数学邀请赛）**

`
这是一项针对美国高中生的具有挑战性的数学考试，被用来评估 LLM 的数学推理和问题解决能力的基准
`

## GPQA Diamond

**一个困难的智力基准，用于测试化学、物理和生物学方面的专业知识**

- 研究生级别的 Google 验证问答基准测试，旨在评估模型对于生物学、物理学和化学领域的深入理解
- 难度高：数据集包含多项选择题，难度极高，即使是追求和持有博士学位的专家，正确率也仅为 65% 左右
- 测试专业知识：用于测试 LLM 在化学、物理和生物学方面的专业知识
- 评估推理能力：旨在衡量模型在需要深度推理和领域专业知识问题上的能力

## LiveCode Bench

**一个代码生成基准，用于评估 LLM 的编程能力**

## Codeforces

**一个在线编程竞赛平台**

- 题目特点：各种难度等级的题目都有，且四位陷阱较多，更侧重与思维，对数据结构和算法的考察相对较弱
- 题目类型：包括动态编程和图形算法等
- Hack 机制：在比赛中，选手可以通过构造数据 hack 掉其他有 bug 的提交来获得奖励

## Pass@k

**一种衡量 LLM 生成多个候选答案时，至少有一个答案正确的概率指标**

### 计算公式

`
Pass@k = (Number of correct answers within top k outputs) / (Total number of prompts)
`

- Pass@10: 表示模型在前 10 次尝试中至少有一次生成了正确的代码

## cons@64

**指的是模型是在 64 次尝试中的综合一致性**

- 衡量模型准确率的指标之一

`
对于 AIME2024 基准测试，使用 64 个样本进行多数投票计算，从而提高评估的稳定性和可靠性
`

## DPO

**Direct Preference Optimization（一种偏好优化方法）**

### 原理

- 避免了首先拟合奖励模型，然后使用强化学习进行优化的传统方法，而是直接优化语言模型以符合人类偏好
- 通过提升偏好响应相对于非偏好响应的相对对数概率来实现优化，从而训练 LLM 以更好地符合人类的期望
- DPO 算法不包括直接的奖励模型和强化学习过程，而是通过指令的偏好数据进行模型微调，将强化学习过程直接转换为大模型 SFT 的过程，从而提升训练更加快速和稳定

### 作用和优势

- 简化流程：简化了 RLHF（基于人类反馈的强化学习）的复杂流程，无需显式奖励模型或强化学习阶段
- 提高效率：提高了训练效率和模型性能
- 稳定且轻量：稳定的、性能和计算成本轻量级的，无需拟合奖励模型，在微调期间从 LM 中采样，或执行显著的超参数调整
- 避免模型退化：结合动态加权机制，避免仅使用概率时遇到的模型退化问题
- 胜过 RLHF：在多个数据集任务上都取得了优于 RLHF 的表现，并提高了摘要和单轮对话的响应质量

### 与 RLHF 的区别

**相同点**

- 目标：都是使模型输出与人类偏好保持一致，提高响应的质量和相关性
- 人类反馈：都是依赖人类反馈来指导优化过程，确保模型的行为与期望的结果相符

**不同点**

- 方法论：RLHF 使用强化学习技术，需要训练奖励模型，然后通过策略梯度等方法优化语言模型；DPO 直接使用静态离线数据集优化模型参数，无需单独的奖励模型
- 复杂性：RLHF 计算密集，需要大量资源来训练奖励模型和执行强化学习更细；DPO 相对简单，计算资源需求较少，更容易实现
- 数据使用：RLHF 可以进行在线和离线训练，能够持续根据新反馈更新模型；DPO 依赖静态数据集，可能限制其适应新反馈的能力
- 优化过程：RLHF 涉及训练多个 LM 和在训练循环中从 LM 策略中采样，产生大量的计算成本。DPO 通过简单的分类目标直接优化最满足偏好的策略，而没有明确的奖励函数或 RL
- 奖励模型：DPO 不需要训练奖励模型，而是通过简化的损失函数直接将人类偏好信息纳入优化过程。RLHF 首先训练一个单独的奖励模型，该模型基于人类反馈来评估语言模型的输出质量，然后使用这个奖励函数来指导语言模型的优化过程

## GGUF

**GPT-Generated Unified Format (一种专为大语言模型设计的二进制文件格式)**

- 旨在解决大模型存储效率、加载速度、兼容性和扩展性等问题
- 由 llama.cpp 创始人 Georgi Gerganov 定义，用于高效地存储和交换大模型预训练结果

### 特点和优势

- 统一性：整合多种优化和量化方法的权重表示，支持未量化的浮点模型和经过量化（如 INT4、 INT8、GPTQ）的模型
- GPU 优化：适用于 GPU 加速的推理任务，能够高效的利用 GPU 的计算资源进行快速推理
- 单文件部署：确保在不依赖外部文件的情况下轻松分发和加载
- 可扩展性：允许向基于 GML 的执行器添加新功能，向 GGUF 模型添加信息，保持与现有模型的兼容性
- mmap 兼容性：可以使用内存映射加载模型，以提高加载和保存速度
- 全面的信息存储：包含加载模型所需的所有数据，不需要用户的额外输入
- 量化兼容性：支持量化，可以缩小模型权重以节省计算资源，而不影响模型能力

## GML

**Graph Modeling Language**

- 一种用于描述图结构和属性的语言，能够以结构化的方式表示知识和关系

### 作用

- 用于图表示：用来表示图结构，并被 LLM 用于执行零样本图推理任务
- 增强 LLM 的能力：利用图结构的信息
- 辅助 SQL 生成：用于生成 SQL 语句，根据待转换文本确定多个转换计划，每个计划包含多个执行步骤，并为每个步骤生成候选 SQL 语句，然后通过路径搜索算法确定最优的 SQL 语句

## VLLM

**Virtual Large Languge Model**

- 一个用于优化大型语言模型推理的高效系统
- 旨在减少计算开销和提高吞吐量来加速 LLM 在推理阶段的性能

**提高推理速度和效率**

- 通过优化内存、连续批处理、CUDA 核心优化和分布式推理支持等手段，提高 LLM 的推理速度和效率

**高效内存管理**

- 采用 PagedAttention 技术，将注意力机制中的键和值存储在不连续的显存空间中，从而减少显存碎片，提高显存效率

**高吞吐服务**

- 支持连续批处理接入和请求，充分利用 GPU 资源，提高吞吐量

**灵活性和易用性**

- 无缝集成流行的 HuggingFace 模型，支持高吞吐服务以及各种解码算法，包括并行采样和束搜索

## 蒙特卡洛估计（Monte Carlo Estimation）

`
强化学习中一种基于完整轨迹（Episode）的回报计算方法，其核心思想是通过对实际采样得到的轨迹回报进行统计平均，来估计状态或动作的价值函数。主要特点是无偏但高方差，适用于无需环境模型的场景
`






## 无偏但高方差

`
统计学和机器学习中的一个重要概念，涉及到估计量的偏差和方差之间的权衡
`

### 无偏

- 一个统计量的期望等于被估计参数的真实值
- 多次重复实验并计算该统计量的平均值，结果将趋近于真实值

### 高方差

- 方差是衡量估计量在不同样本中变动程度的指标
- 高方差意味着估计量对样本数据非常敏感，导致在不同的数据集上可能会产生很大不同的结果


## TD 

**Temporal Difference 时序差分**

- 一种强化学习方法
- 主要用于预测和估计状态-价值函数
- 从不完整的状态序列中学习，通过当前状态和下一个状态的估计值来逐步逼近真实值，而不需要事先知道环境模型

### 关键点

- 核心思想：通过时间上连续状态之间的差分来估计状态价值，利用当前获得的奖励和下一个状态的估值来更新当前状态的价值估值
- TD误差：使用 TD 误差来更新状态价值函数，
![TD](../pics/td.png)
- 更新公式: 状态价值函数的更新公式为 V(State) = V(State) + α * TD(δ)，其中 α 是学习率，控制更新步长
- 与蒙特卡洛方法（MC）的区别：TD 学习不需要完整的序列，每走一步就可以更新价值函数；MC 学习需要完整的序列，每走完完整序列才可以更新价值函数；TD 学习更灵活，适用于无法生成完整序列的场景
- 与动态规划（DP） 的联系：都利用了贝尓曼方程的思想，用后续状态的估计来更新当前状态的价值估计
- 探索策略：通常需要与探索策略结合，如 ε-greedy 策略，在探索和利用之间找到平衡
- 参数选择：性能和稳定性受到学习率，折扣因此等参数的影响，需要根据实际应用选择合适的参数

## PDF

**Probability Density Function (概率密度函数)**

`
概率密度函数是描述连续随机变量概率分布情况的函数，它表示随机变量在某个点或某个范围内的可能性大小
`

### 性质

- 非负性：概率密度函数的值总是非负的
- 规范性：概率密度函数在整个空间上的积分等于 1

### 作用

- 计算概率：通过概率密度函数计算连续型随机变量落在某个区间内的概率，随机变量的取值落在某个区间之内的概率为概率密度函数在这个区域上的积分
- 数据建模与分布拟合：在处理大量数据时，确定数据的分布类型。概率密度函数可用于拟合实际数据，例如通过最大似然估计等方法确定数据是否服从正态分布、均匀分布、指数分布等常见分布
- 描述随机变量的概率分布：用于描述连续型随机变量所服从的概率分布，是概率计算的通用表达
- 计算统计量：概率密度函数可以用来计算随机变量的期望、方差等统计量

## 策略 π(a|s) 函数

`
策略函数 π(a|s) 表示在状态 s 下采取动作 a 的概率
`

- 策略函数 π(a|s) 的定义：本质上是一个概率密度函数（PDF），将从环境观察到的状态 s 作为输入，输出所有动作中每个动作的概率。在多数情况下，该函数以一种概率密度函数 PDF 的形式表现出来，用来模拟现实中做决定的不确定性
- 与基于价值方法的对比：不需要显式地估计每个 {状态，动作} 对的 Q 值，而是通过估计策略函数中的参数，利用训练好的策略模型进行决策

## 策略梯度（Policy Gradient）

`
一种强化学习算法，用于直接优化策略函数，以最大化期望的累积奖励
`

### 定义

```
策略梯度方法通过参数化策略 π(a∣s;θ) 来表示在给定状态 s 下才去动作 a 的概率，其中 θ 是策略的参数

与基于值的方法（Q-learning） 不同，策略梯度直接优化策略，而不是通过估计值函数来间接优化
```

### 作用

- 直接优化策略：允许智能体直接在策略空间中进行搜索，寻找能够最大化期望累积奖励的策略。这种方法适用于复杂的环境和高维动作空间
- 处理连续动作空间：特别适合处理连续动作空间的问题，因为它可以输出一个概率分布，而不仅仅是选择一个离散的动作。这使得它在控制问题（如机器人控制）中表现良好
- 随机策略：在相同状态下，智能体可以以一定概率选择不同的动作，这种随机性有助于探索环境，避免陷入局部最优解

### 主要算法

- Actor-Critic：结合策略梯度和值函数的方法，其中 Actor 负责更新策略，Critic 负责评估当前策略的价值
- REINFORCE 算法：通过采样生成轨迹并计算回报，然后使用这些回报来更新策略参数

## Q-Learning 算法 VS 策略梯度算法

### 基础方法

#### Q-Learning

- 一种基于值函数的强化学习算法，通过学习状态-动作值函数（Q 值）来确定最优策略。智能体通过与环境的交互，更新 Q 表和 Q 函数，以估计在特定状态下采取某个动作的期望回报
- 主要目标是找到一个最优的 Q 函数，使得在任意状态下选择最大 Q 值对应的动作即为最优策略

#### 策略梯度

- 直接优化策略函数，而不是通过估计值函数来间接优化。通过计算策略的梯度，使用梯度上升的方法来调整策略参数，以最大化累积奖励
- 可以学习随机策略，允许在相同状态下以一定概率选择不同的动作

### 策略表示

#### Q-Learning

- 在 Q-Learning 中，策略是隐式的，通过选择 Q 值最大的动作来确定。智能体并不直接学习一个明确的策略，而是通过对 Q 值的更新来间接获得策略

#### 策略梯度

- 策略梯度方法显式的学习一个参数化的策略函数，直接输出在给定状态下采取每个动作的概率。这使得它能够处理复杂和高维的动作空间

### 收敛性与稳定性

#### Q-Learning

- 通过使用 ε-greedy 策略进行探索，即以一定概率选择随机动作，以避免过早收敛到次优策略

#### 策略梯度

- 自然地包含了探索机制，因为它可以输出一个概率分布，使得在相同状态下可以随机选择不同的动作，从而提高探索能力

### 总结

- Q-Learning 更适合离散状态和动作空间
- 策略梯度在处理复杂、高纬问题时表现更好

## KL Divergence

`
在 KL Divergence 公式中， "Log of ratio" 指的是概率分布 p(x) 和 q(x) 的比率的对数，即 log(q(x)/p(x))。这个比率衡量了在真实分布 p 下，使用近似分布 q 来编码数据时的信息损失
`

### 数据与意义

#### 信息损失的度量

- 'Log of ratio' 计算了在真实分布 p 下，使用近似分布 q 所需的额外信息量。具体来说，它表示了在真实分布下，使用近似分布编码样本所需的比特数
- 当 p(x) 相对较大而 q(x) 较小时， log(p(x)/q(x)) 的值会很高，这意味着 q 的编码会损失大量信息

#### 期望值计算

- KL 散度是对所有可能事件的加权平均，其中权重是真实分布 p(x) 决定的。通过对每个事件的 "Log of ratio" 进行加权求和（或积分），KL 散度可以量化整体信息损失
- 公式为：
![KL ratio](../pics/kl_ratio.png)
- 这表明，KL 散度不仅考虑了单个事件的信息损失，还综合考虑了所有可能事件的影响

### 示例

假设有两个概率分布：
- 真实分布 p(x) = 0.8
- 近似分布 p(x) = 0.5

**计算 log of ratio：**

log(p(x)/q(x)) = log(0.8/0.5) ~~ 0.5108

这表示在这个特定情况下，使用近似分布 q 编码来自真实分布 p 的数据时，每个样本平均会损失云 0.5108 比特的信息

## SGD (随机梯度下降)

**Stochastic Gradient Descent**

`
一种在机器学习和优化问题中广泛应用的迭代算法，尤其在处理大规模数据集时表现出高效性，主要用来通过迭代更新参数来最小化目标函数（损失函数）
`

### 原理

- 在机器学习和深度学习中，目标函数的损失函数通常取各个样本损失函数的平均
- 梯度下降法在每次迭代过程中都要对所有样本进行求梯度，因此开销非常大
- 随机采样一个样本更新参数，计算开销就大大降低

### 公式

![sgd](../pics/sgd.png)

### 与其它梯度下降法的比较

- 梯度下降法（Gradient Descent, GD）：使用整个训练数据集来计算梯度，计算得到的是一个标准梯度。对于凸问题，跟定可以达到一个全局最优。担当样本很多时，更新一次要很久
- 小批量梯度下降法（Mini-Batch Gradient Descent, MBGD）：每次更新时用 b 个样本，是一种这种的方法，用一些小样本来近似全部，不仅可以减少计算成本，还可以提高算法稳定性



























## 引用资料

- [关于 DeepSeek 的研究和思考（Archerman Capital）](https://x.com/dotey/status/1883801546934083944?s=46&t=ulYQEDJ7GQSP3RJjsg3CJw)
- [DeepSeek-R1 通过强化学习奖励大语言模型的推理能力](https://mp.weixin.qq.com/s/eU4gg5kXjeN7i5v2fmuUzQ)
