# Noun Explanation

## Bert

**Bidirectional Encoder Representations from Transformers**

### 框架

- 基于 Transformer 编码器部分
- 采用双向编码方式（能够同时考虑上下文的前后信息，从而生成更丰富的词表示）
- 能够同时考虑上下文的前后信息，从而生成更丰富的词表示

### 训练方式

- 主要用于掩码语言模型（masked Language Model, MLM）和下一句的预测（Next Sentence Prediction, NSP）作为训练任务
- MLM 通过随机掩盖输入序列中的一些词来训练模型，从而预测被掩盖的词
- BSP 用于判断两个句子是否连续

### 上下文的理解能力

- 双向特性，在理解整个句子或段落时表现出色，适合需要深入理解文本内容的任务，如情感分析、命名实体识别等

### 应用领域

- 广泛应用于分类、问答系统和句子关系判断等任务


## Gpt

**Generative Pre-trained Transformer**

### 框架

- 基于 Transformer 的解码器部分
- 采用自回归模型（从左到右生成文本，只能依赖于已生成的上下文来预测下一个词）

### 训练方式

- Gpt 使用自回归模型进行训练，专注于预测序列中的下一个词
- 通过在大规模无监督语料上进行训练，逐步生成文本

### 上下文的理解能力

- Gpt 作为单向模型，在生成文本时只能依赖已生成的文本，在处理需要全面理解的任务时可能表现不足
- 文本生成能力较强，适用于对话生成和内容创作等场景

### 应用领域

- 文本生成、机器翻译和对话系统

## BPE

**Byte Pair Encoding**

### 框架

- 一种基于统计的分词算法，最初用于数据压缩
- 通过迭代地合并最频繁的字符对来构建词汇表，减少词汇表的大小
- 减少词汇表大小，保持较高的词汇覆盖率，提高了模型的泛化能力

### 训练方式
- 初始化：将文本中的每个单词分解为字符序列
- 迭代合并：统计所有字符对的频率，选择出频率最高的字符对进行合并
- 跟新词汇表：重复合并过程，直到达到预订的词汇表大小或不再有频繁的字符对

### 应用领域

- 广泛应用于自然语言处理的任务中，如机器翻译、文本生成和语言模型训练

## SFT

**Supervised Fine-Tuning**
- 一种在深度学习和自然语言处理领域中常用的技术
- 通过对预训练模型进行进一步的训练，以提高模型在特定任务上的表现
- 主要用于在预训练模型的基础上，通过有监督的标注数据进一步调整模型参数，使其适应特定任务或领域的需求

### SFT 是什么

- 定义：SFT 是模型预训练的第二阶段（预训练 -> 微调 -> 其它优化）
- 输入数据：使用高质量的标注数据（如问答对、指令-回复对、任务示例），通常由人工标注或经过筛选生成
- 目标：调整预训练模型的参数，使其输出更符合特定任务的要求（如：对话、摘要、分类等）

### SFT 在大模型训练中的作用

#### 任务适配性提升

- 预训练模型（如 GPT、LLaMA） 通过海量无监督数据学习通用语言模型，但缺乏对具体任务的针对性
- SFT 通过标注数据教会模型如何遵循特定指令或格式（如代码生成、回答问题、客服对话），显著提升任务表现

#### 控制输出风格和质量

- 调整输出风格：例如让模型生成更正式、简洁或口语化的内容
- 减少有害/偏见输出：通过标注数据引导模型规避错误、偏见或不安全的内容

#### 解决预训练模型的局限性

- 预训练模型可能对低频任务（如专业领域术语）表现不佳，SFT 通过针对性数据增强其能力
- 修正预训练中未对齐的人类偏好或任务需求的行为（如冗余回复、答非所问）

### SFT 的典型流程

- 预训练模型：使用无监督数据（如互联网文本）训练基础模型（如 GPT-3）
- 构建监督数据集：收集任务相关的标注数据（如人工编写的问答对、指令回复对）
- 微调训练：在标注数据继续训练，调整模型参数（通常学习率较低、避免覆盖预训练的知识）
- 评估和迭代：验证模型在目标任务上的表现，调整数据和训练策略

### 示例应用场景

- 对话系统：用用户-助手对话数据微调，使模型生成更自然的交互回复
- 代码生成：用代码注释-代码对数据训练模型生成符合需求的代码片段
- 内容安全：通过标注的 "安全回复" 数据减少模型的有害输出

## RLHF

**Reinforcement Learning from Human Feeback 基于人类反馈的强化学习** 

### 定义

- 定义：RLHF 是一种结合强化学习和人类反馈的学习方法，通过人类对模型输出的评分或排序，训练模型生成更符合期望的文本
- 适用阶段：通常在大模型完成 预训练（Pretraing）和 监督微调（SFT） 之后使用，属于对齐（Alignment） 阶段的核心技术
- 核心思想：人类无法直接标注所有可能的输出，但可以通过对少量样本的反馈（好坏排序）训练一个奖励模型（Reward Model），再使用该模型代替人类指导强化学习

### RLHF 在 LLM 中的作用

#### 对齐人类偏好（Alignment）

- 问题：预训练或 SFT 后的模型可能生成不友好、有害或偏离用户意图的内容
- RLHF方案： 通过人类反馈定义 "好输出" 的标准（如安全性、有用性、真实性），训练模型对齐这些标准
- 示例：让模型避免生成歧视性言论或虚假信息

#### 提升生成内容的质量

- 优化主观性指标：如流畅性、逻辑性、创造性等难以通过监督学习直接优化的目标
- 示例：让模型生成更生动有趣的故事，或更符合特定场景的对话回复

#### 解决 SFT 的局限性

- SFT 依赖高质量的标注数据，但标注成本高且难以覆盖所有场景
- RLHF 通过少量反馈数据泛化到更广泛的情况，且能处理模糊或多目标的任务（如："既要简洁又要全面"）

#### 控制输出的安全与伦理

- 通过人类反馈明确禁止模型生成暴力、违法或违背道德的内容
- 示例：ChatGPT 通过 RLHF 大幅减少有害回复的概率

#### 数据高效性

- 相比需要海量标注数据的监督学习，RLHF 只需人类多少量输出进行评分或排序，即可训练奖励模型指导优化

### RLHF 的工作流程

**预训练 & SFT**

- 使用无监督数据训练基础模型（如 GPT-3），再通过监督微调（SFT） 初步对齐任务需求

**收集人类反馈**

- 让人类标注员对模型生成的多个输出进行评分或排序（如对同一个问题生成 4 个答案，标注员按照质量排序）

**训练奖励模型**

- 用人类反馈数据训练一个 "奖励模型"，使其能够预测人类对任意输出的偏好程度

**强化学习微调（PRO 算法）**

- 将预训练模型作为策略，奖励模型作为环境，使用强化学习算法（PRO） 优化模型，最大化累积奖励
- 关键点：需约束模型不要过度偏离原始预训练分布（防止 "奖励黑客" 或市区通用性）

### RLHF 的示例应用

- ChatGPT: 通过 RLHF 大幅提升对话的友好性、安全性和实用性
- Claude: 利用 RLHF 避免模型生成政治敏感或偏激的内容
- GPT-4: 结合 RLHF 优化复杂任务（如代码生成、多轮推理）的输出质量

## SFT Vs RLHF

### 核心目标

| SFT | RLHF | 
| :----: | :----: |
| 让模型学会执行特定任务（如问答、翻译）通过标注数据明确 "正确的输出"是什么 | 让模型对齐人类偏好（如安全性、有用性），通过反馈优化"主观质量" 或复杂目标（如 "简洁且全面"） |
| 示例：教模型按格式生成代码 | 示例：让模型避免生成有害内容，或生成更自然的对话回复 |

### 数据使用方式不同

| SFT | RLHF | 
| :----: | :----: |
| 依赖高质量标注的输入-输出对（如问题-答案、指令-回复） | 依赖人类对模型输出的反馈（如对多个输出的排序、评分或偏好标注） |
| 数据特点：明确、具体、任务相关 | 数据特点：主观、抽象、反映人类偏好 |
| 数据量：通常需要大量标注数据 | 数据量：反馈数据量较少，但需要覆盖多样性场景 |

### 训练方式不同

| SFT | RLHF | 
| :----: | :----: |
| 使用监督学习（Supervised Learning），直接通过交叉熵损失优化模型输出与标注答案的一致性 | 使用强化学习（Reinforcement Leaning），通过奖励模型（Reward Model）间接优化模型输出 |
| 训练方式：单阶段，直接调整模型参数 | 训练方式：多阶段（需要先训练奖励模型，再优化策略模型） |
| 优化目标：最小化预测误差 | 优化目标：最大化累积奖励（人类偏好得分） |

### 适用场景差异

| SFT | RLHF | 
| :----: | :----: |
| 任务明确，输出标准清晰：如分类、翻译、结构化生成 | 任务复杂，目标主观或需多目标平衡：如对话安全性、创造性写作、伦理对齐 |
| 示例场景：生成符合格式的 JSON 数据 | 示例场景：避免生成政治敏感内容 |

## GRPO 

**Group Relative Policy Optimization**

- 通过组内相对奖励来优化模型的策略，而不依赖传统的批评模型（critic model）

### GRPO 的核心原理

#### 相对奖励机制

- 通过从当前策略中采样一组输出，并计算这些输出的相对表现来进行优化
- 避免对每个输出进行单独评估的复杂性，简化了训练过程

#### 工作流程

- 输出采样：对于每个任务，GRPO 生成多个可能的答案或推理步骤
- 奖励计算：对这些输出进行评分，计算每个输出的奖励，并根据相对表现确定其优势
- 策略优化：根据相对优势调整策略，使得表现比较好的输出更有可能被生成，而表现比较差的输出被抑制

#### 优势和特点

- 简化训练：由于不需要评判模型，GRPO 显著降低了计算成本和复杂度
- 高效优化：通过组内相对奖励，GRPO 能够在复杂任务中提高模型性能

### 在 LLM 中的应用

`
GRPO 特别适合用于推理任务，如数学推理和代码生成等。在这些任务中，模型需要在多个步骤中逐步生成答案，GRPO 通过优化这些步骤的生成策略，提高了模型在推理过程中的准确性和效率
`

## MoE

**Mixture of Experts**

- 一种先进的神经网络架构
- 通过将多个专门化的子模型（即"专家"）组合在一起，以提高模型的效率和性能

### 核心概念

- 专家（Experts）：每个专家是一个独立的子网络，专注于特定类型的任务和数据处理；例如：在自然语言处理任务中，一个专家可能专注于语法分析，另一个专注于情感分析
- 门控系统（Gating Network）：该组件负责根据输入数据动态选择哪些专家参与处理，这种选择机制使得只有少数专家被激活，从而提高计算效率
- 稀疏激活（Sparse Activation）：在每次计算中，只有一小部分专家被激活，这降低了每次推理所需的计算负载，使得模型能够在较低的资源消耗下处理复杂任务

## RoPE

**Rotary Position Embedding（旋转位置编码）**

- 一种基于 Transformer 模型的位置编码方法
- 将位置信息有效集成到自注意力机制中，以提升模型性能

### 核心原理

- 位置信息编码：通过将位置信息编码为旋转向量，将其与词嵌入向量相加，从而增强模型对位置信息的理解
- 旋转角度计算：根据位置信息计算旋转角度，使得不同位置的词嵌入向量在不同角度上有差异

### 作用

- 集成相对位置信息：能够将显式的相对位置依赖性无缝集成到自注意机制中，使得模型在处理序列时能够更好地捕捉到 token 之间的关系
- 良好的外推性：与传统的绝对位置编码不同，RoPE 允许在推理时处理超过训练长度的输入
- 降低计算复杂度：与线性注意力机制兼容，使得在处理长文本时，计算复杂度 
- 增强模型性能：通过结合绝对和相对位置编码的特点，提升 Transformer 架构在多种任务中的表现，包括自然语言处理、文本生成等领域

## LoRA

**Low-Rank Adaptation**

- 一种用于微调大型预训练模型的技术
- 通过引入低秩矩阵来显著减少微调所需的参数数量和计算资源，同时保持模型的性能

### 核心原理
- 低秩矩阵分解：将一个大的可学习权重矩阵分解为两个低秩矩阵，通过添加两个可学习的低秩矩阵 A 和 B 进行微调：
![lora](../pics/lora.png)
其中：a 是缩放因子，控制 LoRA 部分对输出的影响
- 冻结预训练权重：在微调过程中，LoRA 保持预训练的原始权重不变，仅更新低秩矩阵 A 和 B。这种方法显著减少了需要调整的参数数量，降低了计算成本和内存需求

### 在大语言模型中的作用

- 高效微调：LoRA 能够在对象 GPT、Bert 等大语言模型进行微调时，仅调整 0.1% 的参数，同时保持与全量微调相近的性能。这使得 LoRA 特别适合在计算资源有效的环境中使用
- 跨领域迁移学习：LoRA 可以快速适应新的领域数据，例如从自然语言理解迁移到医学文本的分析，而无需对整个预训练模型重新微调。这种灵活性对于快速响应不同任务需求至关重要
- 低资源设备上的部署：由于 LoRA 仅需存储和加载小规模的低秩矩阵，使得在资源受限的设备上，如边缘计算或移动设备，也能够实现高效的微调和推理
- 多任务学习: LoRA 允许在同一模型中处理多个任务，通过调整不同的低秩矩阵来适应各种不同任务需求，从而提高模型的通用性和适应能力

## MHA Vs MLA

### MHA

**Multi-Head Attention**

- 将注意力机制扩展到多个 "头" 的方法
- 通过过个独立的注意力计算来提取输入数据的多样特征
- 每个头可以关注输入的不同部分，从而增强模型的表达能力

#### MHA 的原理

- 输入：MHA 接收三个变量：查询（Query）、键盘（Key）和值（Value）
- 计算过程：
  - 每个头独立计算查询、键和值的点积，得到注意力分数
  - 对注意力分数进行归一化，得到注意力权重
  - 将注意力权重与值相乘，得到加权和，作为每个头的输出
  - 将所有头的输出拼接起来，得到最终的多头注意力输出
  - 通过线性变换将输出映射到所需的维度

#### 应用

 - MHA 在 Transformer 模型中起着关键作用，广泛应用于机器翻译、文本生成等任务，能够捕捉长距离依赖关系

 ### MLA

**Multi-Head Latent Attention**

#### 定义

- 一种新兴的注意力机制，旨在优化传统 MHA 的性能
- 通过引入潜在空间和动态特征选择来改变注意力计算，提高模型效率和灵活性

#### 原理

- 动态特征选择：MLA 采用灵活的方法来选择关注的特征，通过潜在空间中的投影矩阵来替代固定的分割和重复机制
- 高效计算：通过较少需要处理的数据量，提高计算效率，同时保持模型的性能

#### 应用
- 适用于各种深度学习任务，尤其是在资源受限环境下，如移动设备或嵌入式系统

### 对比

| 特征 | MHA | MLA | 
| :----: | :----: | :----: |
| 基本概念 | 多头注意力机制 | 多头潜在注意力机制 |
| 输入类型 | 查询、键、值 | 查询、键、值 |
| 计算方式 | 独立计算多个头的注意力 | 动态选择与压缩输入信息 |
| 效率 | 计算复杂度较高 | 通过预处理降低计算复杂度 |
| 灵活性 | 固定的头部关注 | 更灵活的特征选择能力 |
| 应用场景 | 广泛应用于 NLP 任务 | 使用于资源受限环境下的任务 |

## CoT

**Chain of Thought**

- 一种用于大语言模型的提示技术
- 旨在提升模型在复杂推理任务中的表现
- 通过引导模型在生成最终答案之间，逐步推导出一系列中间步骤，从而模拟人类的思考过程

### CoT 的作用

- 提升复杂问题解决能力：将复杂问题拆解为多个子问题，使得模型在处理时更具理性和逻辑性，从而提高解决问题的能力
- 增强模型可解释型：通过展示推理过程，CoT 使得模型的决策过程更加透明，用户可以更容易理解模型如何得出结论，并能够识别潜在的错误步骤
- 促进学习和进化：帮助模型更好地理解问题本质，提升学习能力，使得在面对新问题时能够更快适应

### CoT 的示例

**当被问道一个数学问题时，CoT 的推理步骤：**

- 理解问题：首先，模型会解析问题的内容，识别出关键变量和条件
- 分解步骤：模型将复杂问题分解为更简单的子问题。例如：如果问题是计算某个物体的速度，模型可能会首先计算距离和时间
- 逐步推理：对于每个子问题，模型会进行详细的逻辑推导，并解释每一步的原因
- 得出结论：最后，模型整合所有步骤的结果，给出最终的答案

## RL（强化学习）

**Reinforcement Learning**

### 定义

- 一种通过与环境交互来学习最优策略的机器学习方法
- 通过执行动作、观察环境反馈（奖励或惩罚），不断调整自己的策略，以最大化累积奖励
- 与依赖标注数据的监督学习和无标注数据中寻找模式的无监督学习不同，RL 专注于通过试错从直接反馈中学习

### 在大语言模型中的作用

- 优化生成质量：用于微调大语言模型，使其生成的文本更符合人类偏好。例如：通过引入奖励模型，对生成文本进行评分，从而指导模型改进输出质量
- 提高样本效率：大语言模型通常需要大量的数据进行训练，而 RL 能够通过模拟或利用先验知识来提高样本效率，减少对环境交互的需求。这使得模型能够在较少的数据下快速适应新任务
- 多任务学习：通过设计合适的奖励函数， RL 可以促进模型在多个任务之间的泛化能力。例如：在自然语言处理任务中，模型可以同时处理翻译、问答和文本生成等任务
- 人类反馈强化学习（RLHF）：将人类反馈融入到强化学习中的一种方法，通过让人类评估模型的输出并给予奖励，进一步优化模型性能。这种方法在自然语言理解和生成任务中表现尤为突出

## MTP

**Multi-Token Prediction**

- 一种用于大语言模型的训练和推理方法，旨在提高模型效率和性能

### MTP 的作用

- 提高训练效率：通过一次性生成多个 token，MTP 使得模型能够学习更长的 token 依赖关系。这种方法促进模型更好的理解上下文，避免陷入局部决策的学习模式，从而加速模型收敛
- 增强推理速度：在推理阶段, MTP 允许并行预测多个 token，显著提高推理速度。通过减少每次生成时的计算次数，模型能够更快速地响应输入
- 提高泛化能力：MTP 在训练过程中通过多步 token 预测，增强了模型对长距离依赖关系的学习能力。这使得模型能够更好地处理复杂任务，提高其泛化能力
- 优化内存使用：传统的逐个 token 生成方式会频繁范文内存，而 MTP 通过减少这种交互，优化内存使用效率，降低训练和推理过程中的计算瓶颈
- 适应性强：可以灵活应用于不同类型的任务，如文本生成、机器翻译等

## DualPipe

- 一种创新的流水线并行算法，主要用于提高大语言模型训练过程中的效率，尤其是在跨节点的分布式训练场景中
- 通过优化计算和通信的重叠，显著减少了训练时间和资源浪费
- 传统训练信息流水线会产生一些等待时间，有 "流水线气泡"。双重流水线，让一个计算阶段在等待数据传输时可以切换到另一批数据，充分利用空闲时间

## TTS

**Text-to-Speech**

- 将文本转换成语音的深度学习模型

### 原理

- 利用深度学习技术，如卷积神经网络（CNN）、循环神经网络（RNN） 或 Transformer 等，对大量的语言数据和对应的文本进行学习，从而理解文本中的语义信息，并将其转换为相应的语音信号

### 模型的结构

- 文本编码器：负责将输入的文本转换为相应的语义向量，常用的结构包括 RNN 和 Trasformer
- 声学编码器：将语义向量转换为声学特征，如 Mel 频谱，常见结构包括 CNN 和 RNN
- 声学编码器：将声学特征转换为语音波形，常用的解码器包括基于长短时记忆网络（LSTM） 和 Transformer 的解码器

## RNN

**Recurrent Neural Network（循环神经网络）**

### 定义

- 专门处理 "有顺序的数据" 的神经网络，比如一句话、语音、时间序列（比如股票的价格），它的特点是能记住 "前面的信息"，用来影响后面的结果 

### 工作原理
- 输入：RNN 接收一个序列的输入，通常是一个向量序列，每个向量表示序列中的一个元素
- 状态：RNN 内部维护一个 "状态"，用于存储之前的信息
- 计算：对于序列中的每个元素，RNN 执行以下步骤：
  - 计算当前元素与状态的组合，得到新的状态
  - 输出当前元素的预测结果
  - 更新状态，准备处理下一个元素  

## CNN

**Convolutional Neural Network（循环神经网络）**

- 专门处理 "网络状数据" 的神经网络，比如图片、视频
- 用 "局部扫描"  提取关键特征（比如边缘、形状），再组合起来识别整体

### 例子

**如何识别一张图片是不是猫**

- 先扫描图片的局部区域，识别出 "边缘"（比如猫的耳朵的轮廓）
- 再组合边缘，识别出 "形状"（比如猫的耳朵、眼睛）
- 最后综合所有特征，判断这是猫

## LSTM

**Long Short-Term Memory**

- 一种特殊的神经网络，专门用来处理 "有顺序的数据"（比如一句话、 股票的价格走势），它能记住长期信息，解决普通神经网络 "记忆性差" 的问题

### LSTM Vs RNN

- RNN： 像金鱼，只能记住最近几个词，比如 "我爱吃"，可能猜测下一个词是 "饭"
- LSTM: 像人类，能记住更远的上下文，比如 "我从小在安徽萧县长大，所以我爱吃"，这时它会结合 "萧县" 猜测下一个词可能是 "面皮"

### 为什么需要 LSTM

- 问题：普通 RNN 来处理长序列（比如长文章）时，会 "忘记" 开头的内容
- 原因：信息在传递的过程中逐渐消失或爆炸（技术术语： 梯度消失/爆炸）
- LSTM：通过 "门控机制" 决定记住什么，忘记什么，控制信息流

### 三大 "门"

- 遗忘门：决定 "忘记"  哪些旧的信息
- 输入门：决定 "记住" 哪些新信息
- 输出门：决定当前时刻输出什么结果








## 引用资料

- [关于 DeepSeek 的研究和思考（Archerman Capital）](https://x.com/dotey/status/1883801546934083944?s=46&t=ulYQEDJ7GQSP3RJjsg3CJw)
