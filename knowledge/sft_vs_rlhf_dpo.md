# SFT VS RLHF VS DPO

## 核心概念

### SFT (Supervised Fine-Tunning)

- 定义：在预训练模型的基础上，使用高质量标注数据（如人工编写的答案）进行监督式微调，直接优化模型输出与标注数据的匹配度
- 目标：让模型模仿标注数据的行为，适合需要确定性输出任务（如问答、指令遵循）
- 数据需求：需要高质量、成对的输入 - 输出数据（如问答，标准答案）

### RLHF (Reinforcement Learning from Human Feedback)

- 定义：通过强化学习框架，结合人类对生成结果的偏好反馈（如对多个答案排序），训练奖励模型（Reward Model），再用强化学习算法（如 PPO）优化策略模型
- 目标：让模型生成更符合人类主观偏好的内容（如更安全、更流畅）
- 数据需求：需要人类标注偏好数据（如 回答 A、回答 B），以及额外的奖励模型训练步骤

### DPO (Direct Preference Optimization)

- 定义：直接利用偏好数据优化模型，无需显示训练奖励模型，通过数学变换将强化学习目标转换为分类任务，简化 RLHF 的流程
- 目标：与 RLHF 一致，但通过更高效的方式实现偏好对齐
- 数据需求：仅需偏好数据（如 好的问答、差的问答），无需单独训练奖励模型

## 联系和演进

### 递进关系

- SFT 是 RLHF 和 DPO 的前置步骤，提供基础能力（如 ChatGPT 先通过 SFT 微调，再用 RLHF 优化）
- DPO 可视为 RLHF 的数学简化版，通过重新参数化强化学习目标，绕过了显式的奖励模型训练

### 替代关系

- DPO 在效果上接近甚至超过 RLHF，且更易实现，未来可能成为 RLHF 的替代方案

## 典型应用

- SFT：需要确定性输出任务，如法律文档生成，医疗问答
- RLHF：需要平衡复杂偏好的场景，如避免有害内容，生成更自然的对话（如 ChatGPT）
- DPO：资源有限但需快速实现偏好对齐场景，如中小规模模型微调