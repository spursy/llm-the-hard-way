# Proper Noun

## LLM

- Trasformer: Attention is all you need (self-attention)
- Bert: Bidirectional Encoder Representations from Transformers
- Gpt: Generative Pre-trained Transformer
- MLM: Masked Language Model
- NSP: Next Sentence Prediction
- BPE: Byte Pair Encoding
- SFT: Supervised Fine Tunning
- RLHF: Reinforcement Learning from Human Feedback (基于人类反馈的强化学习)
- PRO: Preference Ranking Optimization (偏好排序优化)
- Policy Optimization (策略优化)
- Prompt-based Reinforcement Optimization (基于提示的强化优化)
- Probabilistic Reinforcement  Optimization (概率表示优化)
- Proximal Policy Optimization (近端策略优化)
- GRPO: Group Relative Policy Optimization
- MoE: Mixture of Experts
- MLA: Multi-head Latent Attention (多头潜在注意力)
- MHA: Multi-Head Attention (传统多头注意力)
- RoPE: Rotary Position Embedding（旋转位置编码）
- LoRA: Low-Rank Adaptation
- CoT: Chain of Thought（思维链）
- RL: Reinforcement Learning (强化学习)
- MTP: Multi Token Prediction 
- TTS: Text-to-Speech 
- RNN: Recurrent Neural Network（循环神经网络）
- CNN: Convolutional Neural Network（卷积神经网络）
- LSTM: Long Short-Term Memory




