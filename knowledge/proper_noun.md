# Proper Noun

## LLM

- Trasformer: Attention is all you need (self-attention)
- Bert: Bidirectional Encoder Representations from Transformers
- Gpt: Generative Pre-trained Transformer
- MLM: Masked Language Model
- NSP: Next Sentence Prediction
- BPE: Byte Pair Encoding
- SFT: Supervised Fine Tunning
- RLHF: Reinforcement Learning from Human Feedback (基于人类反馈的强化学习)
- PRO: Preference Ranking Optimization (偏好排序优化)
- Policy Optimization (策略优化)
- Prompt-based Reinforcement Optimization (基于提示的强化优化)
- Probabilistic Reinforcement  Optimization (概率表示优化)
- Proximal Policy Optimization (近端策略优化)
- GRPO: Group Relative Policy Optimization
- MoE: Mixture of Experts
- MLA: Multi-head Latent Attention (多头潜在注意力)
- MHA: Multi-Head Attention (传统多头注意力)
- RoPE: Rotary Position Embedding（旋转位置编码）
- LoRA: Low-Rank Adaptation
- CoT: Chain of Thought（思维链）
- RL: Reinforcement Learning (强化学习)
- MTP: Multi Token Prediction 
- TTS: Text-to-Speech 
- RNN: Recurrent Neural Network（循环神经网络）
- CNN: Convolutional Neural Network（卷积神经网络）
- LSTM: Long Short-Term Memory
- MCTS：Monte Carlo Tree Search (蒙特卡洛搜索树)
- UCT: Upper Confidence Bound for Trees (蒙特卡洛算法中的核心算法策略)
- Beam Search（束搜索）
- Process-Based Reward Modek (基于过程的模型奖励)
- Cross Entropy Loss（交叉损失函数）
- MSE: Mean Squared Error（均方误差）
- KL 散度：Kullback-Leibler Divergence
- FFN: Feed-Forwrd Network（前馈神经网络）
- Residual Connection: (残差连接)






