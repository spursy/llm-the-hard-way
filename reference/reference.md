# Reference

## vLLM

### 什么是 vLLM

`
vLLM（Vectorized Large Language Model Serving System）是由加州大学伯克利分校团队开发的高性能、易扩展的大语言模型推理引擎。它专注于通过创新的内存管理和计算优化技术，实现高吞吐、低延迟、低成本的模型服务
`

### 核心特点

**高性能**

- 支持分布式推理，能高效利用多机多卡资源

**显存优化**

- 从用 PageAttention 内存管理技术，显著提升 GPU 显存利用率

**多场景适配**

- 无论是低延迟在线服务，还是资源受限的边缘部署，vLLM 都能提供卓越的性能表现


### vLLM vs Ollama

| 对比维度 | Ollama | vLLM | 备注 |
|:-----:|:---------:|:--------:|:--------:|  
| 量化与压缩策略 | 默认采用 4-bit/8-bit 量化，显存占用降至 25%-50% | 默认使用 FP16/BF16 精度，保留完整参数精度 | Ollama 牺牲精度换内存，vLLM 牺牲显存换计算效率 |
| 优化目标 | 轻量化和本地部署，动态加载模块分块，按需使用显存 | 高吞吐量、低延迟，预加载完整模型到显存，支持高并发 | Ollama 适合单任务，vLLM 适合批量任务 |
| 显存管理机制 | 分块加载 + 动态缓存，仅保留必要参数和激活值 | PagedAttention + 全量预加载，保留完整参数和中间激活值 | vLLM 显存占用为 Ollama 的 2-5 倍 |
| 硬件适配 | 针对消费级 GPU（如 RTX 3060）优化，显存需求低 | 依赖专业级（如 A100/H100），需多卡并行或分布式部署 | Ollama 可在 24 GB 显存运行 32 GB 模型，vLLM 需至少 64 GB | 
| 性能与资源平衡 | 显存占用低，但推理速度较慢（适合轻量级应用）| 显存占用高，但吞吐量高（适合企业级服务）| 量化后 Ollama 速度可提升，但仍低于 vLLM |
| 适用场景 | 个人开发、本地测试、轻量级应用 | 企业级 API 服务，高并发推理、大规模部署 | 根据显存的性能需求选择框架 |

**Ollama 更适合个人开发和轻量级应用，而 vLLM 则更适合企业级服务和高并发场景**

### DeepSeek-R1-Distill-Qwen-32B 在 vLLM 和 Ollama 的对比

| 指标 | Ollama（4-bit） | vLLM（FP16） | 说明 |
|:-----:|:---------:|:--------:|:--------:|  
| 显存占用 | 19-24 GB | 64-96 GB | Ollama 通过 4-bit 量化压缩参数，vLLM 需保留完整 FP 16 参数和激活值 |
| 存储空间 | 20 GB | 64 GB | Ollama 存储量化后模型，vLLM 存储原始 FP16 精度模型 |
| 推理速度 | 较低（5 - 15 token/s）| 中高（30-60 token/s） | Ollama 因量化计算效率较低，vLLM 通过批处理和并行优化提升吞吐量 |
| 硬件门槛 | 高端消费级 GPU（>= 24 GB）| 多卡专业级 GPU（如 2 * A00 80GB）| Ollama 勉强单卡运行，vLLM 需多卡并行或分布式部署 |



**Reference**

- [vLLM: 高性能大语言模型推理引擎全面解析](https://mp.weixin.qq.com/s/JPWEsyuJ3Rsw6IgPICzENA)