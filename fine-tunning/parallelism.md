# Parallelism

## Prefill & Decode

`
在分布式并行策略中，Prefill（预填充）和 Decode（解码）是两种针对序列生成任务（如自然语言生成、语音合成等）的并行优化技术，尤其是在基于 Transformer 大大模型推理或训练中广泛应用
`

### Prefill

#### 定义

Prefill 阶段指预先处理输入序列（Prompt）并生成中间状态（如 Key-Value 缓存）的过程

`
例如，在生成文本时，输入一个提示词（如"中国的首都是"），Prefill 会先计算该提示词对应的隐藏状态，为后续自回归生成做准备
`

#### 并行策略与作用

**1. 并行计算输入序列**

- 输入序列（Prompt）的所有 Token 会被一次性并行处理，利用矩阵乘法并行性加速计算
- 例如：输入长度为 N 的序列，Prefill 阶段通过并行计算所有位置的注意力机制和 FFN 层，生成 N 个 Token 的中间状态

**2. 生成 Key-Value（KV）缓存**

- 在 Transformer 的解码器中，Prefill 阶段会缓存每个注意力头的 Key 和 Value 矩阵（KV Cache），避免后续生成时重复计算，显著减少推理延迟

**3. 分布式优化**

- 数据并行：将不同输入样本分配到不同设备并行处理
- 模型并行：拆分模型层到不同设备，适用于大模型（如 GPT-3）
- 动态分块（chunking）：堆场输入序列分块并行计算，环节显存压力

### Decode

#### 定义

Decode 阶段指自回归生成输出序列的过程，每次生成一个 Token，并依赖前序生成的 Token 逐步推进。

`
例如，输入 "中国的首都是"，模型逐步生成 "北" --> "京"（每次依赖前一步的输出）
`

#### 并行策略与作用

**1. 自回归生成的挑战**

- Decode 阶段需串行生成 Token (每一步依赖上一步结果)，天然难以并行化
- 传统方法中，每个 Token 生成需重新计算所有中间状态，导致计算冗余

**2. 优化策略**

- KV 缓存重用

`
利用 Prefill 阶段缓存的 Key-Value 矩阵，后续生成仅计算新 Token 的注意力，避免重复计算历史 Token 的中间状态
`

- 并行化 Token 生成

  - 批处理 (Batching): 同时对多个输入序列进行解码（如同时处理 10 个用户的生成请求）
  - 推理解码 (Speculative Decoding): 通过小模型预测多个候选 Token，大模型并行验证，加速生成速度

 - 内存优化（通过内存共享或压缩技术，减少 KV 缓存的显存占用）

 **3. 分布式优化**

 - 流水线并行：将生成步骤拆分为多阶段，不同设备处理不同阶段（如设备 A 处理第一步，设备 B 处理第二步）
 - 张量并行：拆分注意力头或 FFN 层的不同设备（如 Megaton-LM）




### Prefill 与 Decode 的协同作用

#### Prefill 为 Decode 提供加速基础

- 通过一次性预计算输入序列的中间状态（KV 缓存），使后续 Decode 阶段仅需增量计算，极大减少单步生成时间

`
例如：输入长度为 100 的 prompt，Prefill 阶段并行处理所有 Token 并缓存 KV，Decode 时每次仅需计算一个新的 Token 的注意力
`

#### 资源分配权衡

- Prefill 阶段计算稠密，需高算力并行处理；Decode 阶段内存带宽敏感，需优化缓存和通信开销
- 在分布式系统中，Prefill 可能占用更多设备资源，而 Decode 需保证低延迟和高吞吐

#### 端到端优化案例

- vLLM 框架：通过 PageAttention 技术高效管理 KV 缓存，支持动态批处理，同时优化 Prefill 和 Decode 阶段的显存利用率
- FlashAttention: 加速 Prefill 阶段的注意力计算，减少显存访问次数

### 总结

- Prefill: 通过并行计算输入序列并缓存中间状态，为后续生成提供加速基础，适用于预训练和初始化
- Decode: 利用缓存和并行策略突破自回归生成的串行瓶颈，提升生成效率，适用于逐步输出场景
- 分布式优化：结合数据并行，模型并行，流水线并行等技术，平衡计算、显存和通信开销，是提升大模型推理性能的核心手段